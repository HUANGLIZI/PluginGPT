# PluginGPT
PluginGPT: Rethinking the Role of GPT Plugins

## OpenAI GPT4/3.5-Based
1. VideoChat: Chat-Centric Video Understanding [Arxiv](https://arxiv.org/abs/2305.06355), [Code](https://github.com/OpenGVLab/Ask-Anything)
2. ChatCAD+: Towards a Reliable and Universal Interactive CAD using LLMs [Arxiv](https://arxiv.org/abs/2305.15964), [Code](https://github.com/zhaozh10/ChatCAD)
3. MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action [Arxiv](https://arxiv.org/abs/2303.11381), [Code](https://github.com/microsoft/MM-REACT)
4. Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models [Arxiv](https://arxiv.org/abs/2303.04671), [Code](https://github.com/microsoft/TaskMatrix)
## Pretrain Model-Based (LLaMA, Vicuna, LLaVA, OpenFlamingo, ChatGLM)
### LLaMA-Based
1. Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding [Arxiv](https://arxiv.org/abs/2306.02858), [Code](https://github.com/DAMO-NLP-SG/Video-LLaMA)
2. mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality [Arxiv](https://arxiv.org/abs/2304.14178), [Code](https://github.com/x-plug/mplug-owl)
3. ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge [Arxiv](https://arxiv.org/abs/2303.14070), [Code](https://github.com/Kent0n-Li/ChatDoctor)
4. HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge [Arxiv](https://arxiv.org/abs/2304.06975), [Code](https://github.com/scir-hi/huatuo-llama-med-chinese)
5. LMEye: An Interactive Perception Network for Large Language Models [Arxiv](https://arxiv.org/abs/2305.03701), [Code](https://github.com/YunxinLi/LingCloud)
6. Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks [Arxiv](https://arxiv.org/abs/2305.14201), [Code](https://github.com/liutiedong/goat)
7. LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models [Arxiv](https://arxiv.org/abs/2304.01933), [Code](https://github.com/AGI-Edgerunners/LLM-Adapters)
8. LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention [Arxiv](https://arxiv.org/abs/2303.16199), [Code](https://github.com/opengvlab/llama-adapter)
9. SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities [Arxiv](https://arxiv.org/abs/2305.11000), [Code](https://github.com/0nutation/speechgpt)
10. EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought [Arxiv](https://arxiv.org/abs/2305.15021), [Code](https://github.com/EmbodiedGPT/EmbodiedGPT_Pytorch)
### Vicuna-Based (also LLaMA-Based)
1. MiniGPT-4: Enhancing Vision-language Understanding with Advanced Large Language Models [Arxiv](https://arxiv.org/abs/2304.10592), [Code](https://github.com/vision-cair/minigpt-4)
2. XrayGPT: Chest Radiographs Summarization using Large Medical Vision-Language Models [Arxiv](https://arxiv.org/abs/2306.07971), [Code](https://github.com/mbzuai-oryx/XrayGPT)
3. Path to Medical AGI: Unify Domain-specific Medical LLMs with the Lowest Cost** [Arxiv](https://arxiv.org/abs/2306.10765), [Code](https://github.com/JoshuaChou2018/MedAGI)(**The Same First Author)
4. SkinGPT-4: An Interactive Dermatology Diagnostic System with Visual Large Language Model** [MedRxiv](https://www.medrxiv.org/content/10.1101/2023.06.10.23291127v1), [Code](https://github.com/JoshuaChou2018/SkinGPT-4)(**The Same First Author)
6. ProteinChat: Towards Enabling ChatGPT-Like Capabilities on Protein 3D Structures [TechRixv](https://www.techrxiv.org/articles/preprint/ProteinChat_Towards_Achieving_ChatGPT-Like_Functionalities_on_Protein_3D_Structures/23120606), [Code](https://github.com/UCSD-AI4H/proteinchat)
7. PandaGPT: One Model to Instruction-Follow Them All [Arxiv](https://arxiv.org/abs/2305.16355), [Code](https://github.com/yxuansu/PandaGPT)
8. GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction [Arxiv](https://arxiv.org/abs/2305.18752), [Code](https://github.com/StevenGrove/GPT4Tools)
### LLaVA-Based (also LLaMA-Based)
1. LLaVA-Med: Large Language and Vision Assistant for BioMedicine [Arixv](https://arxiv.org/abs/2306.00890), [Code](https://github.com/microsoft/LLaVA-Med)
### OpenFlamingo-Based (also LLaMA-Based)
1. MultiModal-GPT: A Vision and Language Model for Dialogue with Humans [Arxiv](https://arxiv.org/abs/2305.04790), [Code](https://github.com/open-mmlab/Multimodal-GPT)
2. Otter: A Multi-Modal Model with In-Context Instruction Tuning [Arxiv](https://arxiv.org/abs/2305.03726), [Code](https://github.com/Luodian/otter)
### ChatGLM-Based
1. X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages [Arxiv](https://arxiv.org/abs/2305.04160), [Code](https://github.com/phellonchen/X-LLM)
