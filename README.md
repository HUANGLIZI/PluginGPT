# PluginGPT: Rethinking the Role of GPT Plugins

## OpenAI GPT4/3.5-Based
1. VideoChat: Chat-Centric Video Understanding [Arxiv](https://arxiv.org/abs/2305.06355), [Code](https://github.com/OpenGVLab/Ask-Anything)
2. ChatCAD+: Towards a Reliable and Universal Interactive CAD using LLMs [Arxiv](https://arxiv.org/abs/2305.15964), [Code](https://github.com/zhaozh10/ChatCAD)
3. MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action [Arxiv](https://arxiv.org/abs/2303.11381), [Code](https://github.com/microsoft/MM-REACT)
4. Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models [Arxiv](https://arxiv.org/abs/2303.04671), [Code](https://github.com/microsoft/TaskMatrix)
5. Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language (Optional GPT3.5-Based) [Arxiv](https://arxiv.org/abs/2306.16410v1), [Code](https://github.com/ContextualAI/lens)
5. SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization [Arxiv](https://arxiv.org/pdf/2306.17384.pdf), [Code](https://github.com/Raghav1606/SummQA)
5. A Study of Generative Large Language Model for Medical Research and Healthcare  [Arxiv](https://arxiv.org/ftp/arxiv/papers/2305/2305.13523.pdf), [Code](https://github.com/uf-hobi-informatics-lab/NLPreprocessing)
5. SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs   [Arxiv](https://arxiv.org/pdf/2306.17842.pdf), [Code](https://github.com/google-research/magvit/ projects/spae)
## Pretrain Model-Based (LLaMA, Vicuna, Alpaca, LLaVA, OpenFlamingo, PaLM, ChatGLM)
### LLaMA-Based
1. Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding [Arxiv](https://arxiv.org/abs/2306.02858), [Code](https://github.com/DAMO-NLP-SG/Video-LLaMA)
2. mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality [Arxiv](https://arxiv.org/abs/2304.14178), [Code](https://github.com/x-plug/mplug-owl)
3. ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge [Arxiv](https://arxiv.org/abs/2303.14070), [Code](https://github.com/Kent0n-Li/ChatDoctor)
4. HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge [Arxiv](https://arxiv.org/abs/2304.06975), [Code](https://github.com/scir-hi/huatuo-llama-med-chinese)
5. LMEye: An Interactive Perception Network for Large Language Models [Arxiv](https://arxiv.org/abs/2305.03701), [Code](https://github.com/YunxinLi/LingCloud)
6. Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks [Arxiv](https://arxiv.org/abs/2305.14201), [Code](https://github.com/liutiedong/goat)
7. LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models [Arxiv](https://arxiv.org/abs/2304.01933), [Code](https://github.com/AGI-Edgerunners/LLM-Adapters)
8. LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention [Arxiv](https://arxiv.org/abs/2303.16199), [Code](https://github.com/opengvlab/llama-adapter)
9. SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities [Arxiv](https://arxiv.org/abs/2305.11000), [Code](https://github.com/0nutation/speechgpt)
10. EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought [Arxiv](https://arxiv.org/abs/2305.15021), [Code](https://github.com/EmbodiedGPT/EmbodiedGPT_Pytorch)
11. Extending Context Window of Large Language Models via Positional Interpolation [Arxiv](https://arxiv.org/abs/2306.15595)
11. Clinical Camel: An Open-Source Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding [Arxiv](https://arxiv.org/pdf/2305.12031.pdf), [Code](https://github.com/bowang-lab/clinical-camel)
11. Dr. LLaMA: Improving Small Language Models on PubMedQA via Generative Data Augmentation [Arxiv](https://arxiv.org/pdf/2305.07804.pdf), [Code](https://github.com/zguo0525/Dr.LLaMA)
### Vicuna-Based (also LLaMA-Based)
1. MiniGPT-4: Enhancing Vision-language Understanding with Advanced Large Language Models [Arxiv](https://arxiv.org/abs/2304.10592), [Code](https://github.com/vision-cair/minigpt-4)
2. XrayGPT: Chest Radiographs Summarization using Large Medical Vision-Language Models [Arxiv](https://arxiv.org/abs/2306.07971), [Code](https://github.com/mbzuai-oryx/XrayGPT)
3. Path to Medical AGI: Unify Domain-specific Medical LLMs with the Lowest Cost [Arxiv](https://arxiv.org/abs/2306.10765), [Code](https://github.com/JoshuaChou2018/MedAGI)
4. SkinGPT-4: An Interactive Dermatology Diagnostic System with Visual Large Language Model [MedRxiv](https://www.medrxiv.org/content/10.1101/2023.06.10.23291127v1), [Code](https://github.com/JoshuaChou2018/SkinGPT-4)
6. ProteinChat: Towards Enabling ChatGPT-Like Capabilities on Protein 3D Structures [TechRixv](https://www.techrxiv.org/articles/preprint/ProteinChat_Towards_Achieving_ChatGPT-Like_Functionalities_on_Protein_3D_Structures/23120606), [Code](https://github.com/UCSD-AI4H/proteinchat)
7. PandaGPT: One Model to Instruction-Follow Them All [Arxiv](https://arxiv.org/abs/2305.16355), [Code](https://github.com/yxuansu/PandaGPT)
8. GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction [Arxiv](https://arxiv.org/abs/2305.18752), [Code](https://github.com/StevenGrove/GPT4Tools)
### Alpaca-Based (also LLaMA-Based)
1. Radiology-GPT: A Large Language Model for Radiology  [Arxiv](https://arxiv.org/pdf/2306.08666.pdf),  [Code](https://github.com/zl-liu/Radiology-GPT), [Demo](https://huggingface.co/spaces/allen-eric/radiology-gpt)
### LLaVA-Based (also LLaMA-Based)
1. LLaVA-Med: Large Language and Vision Assistant for BioMedicine [Arixv](https://arxiv.org/abs/2306.00890), [Code](https://github.com/microsoft/LLaVA-Med)
2. KOSMOS-1: Language Is Not All You Need: Aligning Perception with Language Models [Arxiv](https://arxiv.org/abs/2302.14045), [Code](https://github.com/microsoft/unilm)
3. KOSMOS-2: Grounding Multimodal Large Language Models to the World [Arxiv](https://arxiv.org/abs/2306.14824), [Code](https://github.com/microsoft/unilm)
4. LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding [Arxiv](https://arxiv.org/abs/2306.17107), [Code](https://llavar.github.io/)
### OpenFlamingo-Based (also LLaMA-Based)
1. MultiModal-GPT: A Vision and Language Model for Dialogue with Humans [Arxiv](https://arxiv.org/abs/2305.04790), [Code](https://github.com/open-mmlab/Multimodal-GPT)
2. Otter: A Multi-Modal Model with In-Context Instruction Tuning [Arxiv](https://arxiv.org/abs/2305.03726), [Code](https://github.com/Luodian/otter)
### PaLM-Based
1. Google AI PaLM 2 [Arxiv](https://arxiv.org/abs/2305.10403), [Demo](https://ai.google/discover/palm2)
2. AudioPaLM: A Large Language Model That Can Speak and Listen [Arxiv](https://arxiv.org/abs/2306.12925v1), [Demo](https://google-research.github.io/seanet/audiopalm/examples/)
3. Large Language Models Encode Clinical Knowledge [Arxiv](https://arxiv.org/abs/2212.13138), [Demo](https://sites.research.google/med-palm/)
### ChatGLM-Based
1. X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages [Arxiv](https://arxiv.org/abs/2305.04160), [Code](https://github.com/phellonchen/X-LLM)
2. OphGLM: Training an Ophthalmology Large Language-and-Vision Assistant based on Instructions and Dialogue [Arxiv](https://arxiv.org/abs/2306.12174), [Code](https://github.com/ML-AILab/OphGLM)
3. VisualGLM-6B [Code](https://github.com/THUDM/VisualGLM-6B)
4. SEEChat [Code](https://github.com/360CVGroup/SEEChat)
5. ChatGLM2-6B [Code](https://github.com/THUDM/ChatGLM2-6B)
